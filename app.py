# ============================================
# ğŸ¤– Chatbot AI â€” Ù†Ø¸Ø§Ù… Ø£Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø© Ø°ÙƒÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Embeddings + Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
# ============================================

from flask import Flask, request, jsonify
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import NearestNeighbors
import google.generativeai as genai
import json, os, re, requests
from bs4 import BeautifulSoup

# --------------------------------------------
# âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
# --------------------------------------------
app = Flask(__name__)

# Ù…ÙØªØ§Ø­ Gemini API
genai.configure(api_key="AIzaSyBEeidGnK_uyf9ikJWW9elsAgDdz8t09oA")

# Ù…Ø³Ø§Ø± Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
FAQ_PATH = os.path.join(os.path.dirname(__file__), "faq.json")

# Ù†Ù…ÙˆØ°Ø¬ Embeddings Ø®ÙÙŠÙ ÙˆØ³Ø±ÙŠØ¹
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# --------------------------------------------
# âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ®ØµÙŠØµ
# --------------------------------------------
TOP_K = 5
EMB_WEIGHT = 0.7
TOKEN_WEIGHT = 0.3
COMBINED_THRESHOLD = 0.60

# --------------------------------------------
# ğŸš« Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© (Stopwords)
# --------------------------------------------
ARABIC_STOPWORDS = {
    "ÙÙŠ", "Ù…Ù†", "Ù…Ø§", "Ù‡ÙŠ", "Ù…Ø§Ù‡ÙŠ", "Ù…Ø§ Ù‡ÙŠ", "Ù„Ù…", "Ø¹Ù†", "Ø¹Ù„Ù‰", "Ùˆ", "Ø§Ùˆ", "Ø£Ùˆ",
    "Ù‡Ù„", "ÙƒÙŠÙ", "Ø£ÙŠÙ†", "ÙƒÙ…", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙƒÙˆÙ†", "ÙŠÙƒÙˆÙ†", "Ù‡Ùˆ", "Ù‡ÙŠ", "Ø¥Ù„Ù‰", "Ø¨"
}

# --------------------------------------------
# ğŸ§  Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„
# --------------------------------------------
questions, answers, token_sets = [], [], []
nn_model = NearestNeighbors(n_neighbors=1, metric="cosine")

# --------------------------------------------
# ğŸ§¹ Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
# --------------------------------------------

def remove_diacritics(text):
    return re.sub(r'[\u0610-\u061A\u064B-\u065F\u06D6-\u06ED]', '', text)

def normalize_ar(text):
    t = text.lower()
    t = remove_diacritics(t)
    t = re.sub(r'[^\u0600-\u06FF\s]', ' ', t)
    return re.sub(r'\s+', ' ', t).strip()

def tokens_from_text(text):
    t = normalize_ar(text)
    return [w for w in t.split() if w and w not in ARABIC_STOPWORDS]

def token_overlap_score(q_tokens, c_tokens):
    if not c_tokens:
        return 0.0
    return len(set(q_tokens) & set(c_tokens)) / max(len(c_tokens), 1)

# --------------------------------------------
# ğŸ’¾ ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# --------------------------------------------
def load_faq_data():
    if not os.path.exists(FAQ_PATH):
        return []
    try:
        with open(FAQ_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return []

def build_index_from_memory():
    global nn_model
    if not questions:
        return
    embeddings = embedder.encode(questions, show_progress_bar=False)
    nn_model = NearestNeighbors(n_neighbors=min(len(questions), TOP_K), metric="cosine")
    nn_model.fit(embeddings)

def initialize_memory():
    global questions, answers, token_sets
    data = load_faq_data()
    questions[:] = [d["question"] for d in data]
    answers[:] = [d["answer"] for d in data]
    token_sets[:] = [tokens_from_text(q) for q in questions]
    if questions:
        build_index_from_memory()
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(questions)} Ø³Ø¤Ø§Ù„ ÙˆØ¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø³Ø¦Ù„Ø© Ù…Ø­ÙÙˆØ¸Ø© Ø¨Ø¹Ø¯.")

initialize_memory()

# --------------------------------------------
# âœï¸ Ø­ÙØ¸ Ø£Ùˆ ØªØ­Ø¯ÙŠØ« Ø³Ø¤Ø§Ù„/Ø¥Ø¬Ø§Ø¨Ø©
# --------------------------------------------
def save_or_update_qa(question, answer):
    data = load_faq_data()
    q_tokens = tokens_from_text(question)
    found_idx = None

    for i, q in enumerate(questions):
        if token_overlap_score(q_tokens, token_sets[i]) >= 0.6:
            found_idx = i
            break

    if found_idx is not None:
        old_q = questions[found_idx]
        for item in data:
            if item["question"].strip() == old_q.strip():
                item["answer"] = answer
                break
        answers[found_idx] = answer
    else:
        data.append({"question": question, "answer": answer})
        questions.append(question)
        answers.append(answer)
        token_sets.append(q_tokens)

    with open(FAQ_PATH, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    build_index_from_memory()

# --------------------------------------------
# ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„ØªØ±Ø¬Ù…Ø©
# --------------------------------------------
def get_best_answer(user_input):
    """
    Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙØ¶Ù„ Ø¥Ø¬Ø§Ø¨Ø© â€” Ù…Ø¹ ØªØ±Ø¬Ù…Ø© Ø°ÙƒÙŠØ©:
    - ÙŠØªØ±Ø¬Ù… Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø¨Ø­Ø«.
    - Ø¨Ø¹Ø¯ Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ ÙŠØªØ±Ø¬Ù… Ø§Ù„Ø±Ø¯ Ø¥Ù„Ù‰ Ù†ÙØ³ Ù„ØºØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….
    - ÙŠØ±Ø¬Ø¹ ÙÙ‚Ø· Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù…Ù‚Ø¯Ù…Ø© Ø£Ùˆ Ø´Ø±Ø­.
    """
    global last_added_question
    original_text = user_input
    answer = ""

    # ---------------------------
    # ğŸ”¹ ØªØ­Ø¯ÙŠØ¯ Ù„ØºØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    # ---------------------------
    detected_lang = "Arabic"
    try:
        model = genai.GenerativeModel("models/gemini-2.5-pro")
        resp = model.generate_content(
            f"Detect the language of this text only. Reply with one word like: Arabic, English, French, etc.\n\n{user_input}"
        )
        detected_lang = resp.text.strip().capitalize()
        print(f"ğŸŒ Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {detected_lang}")
    except Exception as e:
        print("âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„ØºØ©:", e)

    # ---------------------------
    # ğŸ”¹ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø¨Ø­Ø« ÙÙ‚Ø·
    # ---------------------------
    translated_for_search = user_input
    if detected_lang.lower() != "arabic":
        try:
            model = genai.GenerativeModel("models/gemini-2.5-pro")
            prompt = (
                "Translate the following text to Arabic. "
                "Reply ONLY with the translated Arabic text, no explanations, no notes, no markdown:\n\n"
                f"{user_input}"
            )
            resp = model.generate_content(prompt)
            translated_for_search = re.sub(
                r"(?i)(here is the translation|translation|of course|sure|the answer is|:)",
                "",
                resp.text.strip(),
            ).strip()
            print(f"ğŸŒ [ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø³Ø¤Ø§Ù„] {user_input} â†’ {translated_for_search}")
        except Exception as e:
            print("âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø³Ø¤Ø§Ù„:", e)

    # ---------------------------
    # ğŸ” Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    # ---------------------------
    user_toks = tokens_from_text(translated_for_search)

    if not questions:
        answer = "Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ø­Ø§Ù„ÙŠØ§Ù‹ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ²ÙˆÙŠØ¯ÙŠ Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©."
        save_or_update_qa(translated_for_search, answer)
    else:
        q_vec = embedder.encode([translated_for_search])
        k = min(TOP_K, len(questions))
        dist, idxs = nn_model.kneighbors(q_vec, n_neighbors=k)

        best_idx, best_score = None, -1
        for rank, cand_idx in enumerate(idxs[0]):
            emb_sim = 1 - dist[0][rank]
            tok_overlap = token_overlap_score(user_toks, token_sets[cand_idx])
            combined = EMB_WEIGHT * emb_sim + TOKEN_WEIGHT * tok_overlap
            if combined > best_score:
                best_score = combined
                best_idx = cand_idx

        if best_idx is not None and best_score >= COMBINED_THRESHOLD:
            answer = answers[best_idx]
        else:
            answer = "Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ø­Ø§Ù„ÙŠØ§Ù‹ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ²ÙˆÙŠØ¯ÙŠ Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©."

    # ---------------------------
    # ğŸ”¹ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¥Ù„Ù‰ Ù†ÙØ³ Ù„ØºØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    # ---------------------------
    final_answer = answer
    if detected_lang.lower() != "arabic":
        try:
            model = genai.GenerativeModel("models/gemini-2.5-pro")
            prompt = (
                f"Translate the following Arabic text to {detected_lang}. "
                "Reply ONLY with the translated text itself, no explanations, no markdown, no intro phrases:\n\n"
                f"{answer}"
            )
            resp = model.generate_content(prompt)
            translated_answer = re.sub(
                r"(?i)(here is the translation|translation|of course|sure|the answer is|:)",
                "",
                resp.text.strip(),
            ).strip()
            if translated_answer:
                print(f"ğŸŒ [ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©] {answer} â†’ {translated_answer}")
                final_answer = translated_answer
        except Exception as e:
            print("âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:", e)

    # ---------------------------
    # ğŸ’¾ Ù†Ø­ÙØ¸ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
    # ---------------------------
    try:
        save_or_update_qa(translated_for_search, answer)
    except Exception as e:
        print("âš ï¸ ÙØ´Ù„ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­ÙØ¸:", e)

    return final_answer

# --------------------------------------------
# ğŸ’¬ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© (API)
# --------------------------------------------
@app.route("/chat", methods=["POST"])
def chat():
    user_input = request.json.get("message", "")
    session_id = request.json.get("session_id", "default")

    reply = get_best_answer(user_input)
    return jsonify({"reply": reply})

# --------------------------------------------
# ğŸš€ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù…
# --------------------------------------------
if __name__ == "__main__":
    app.run(port=5000, debug=True)

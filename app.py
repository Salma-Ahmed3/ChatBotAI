# ============================================
# ğŸ¤– Chatbot AI â€” Ù†Ø¸Ø§Ù… Ø£Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø© Ø°ÙƒÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Embeddings + Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
# ============================================

# ğŸ§© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
from flask import Flask, request, jsonify              # Ù„Ø¥Ù†Ø´Ø§Ø¡ API Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Flask
from sentence_transformers import SentenceTransformer  # Ù„ØªÙˆÙ„ÙŠØ¯ Embeddings Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ
from sklearn.neighbors import NearestNeighbors         # Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù‚Ø±Ø¨ Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
import google.generativeai as genai                    # Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© Gemini API (Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ù† Google)
import json, os, re, requests                          # Ù…ÙƒØªØ¨Ø§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ù…Ù„ÙØ§Øª ÙˆØ§Ù„Ù†ØµÙˆØµ
from bs4 import BeautifulSoup                          # Ù„ØªØ­Ù„ÙŠÙ„ HTML (Ù…Ù…ÙƒÙ† ØªØ³ØªØ®Ø¯Ù… Ù„Ø§Ø­Ù‚Ù‹Ø§)
from bidi.algorithm import get_display                 # Ù„ØªØµØ­ÙŠØ­ Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…Ù† Ø§Ù„ÙŠÙ…ÙŠÙ† Ù„Ù„ÙŠØ³Ø§Ø±
from flask import send_from_directory                  # Ù„Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„ÙØ§Øª Ø«Ø§Ø¨ØªØ© Ø²ÙŠ favicon.ico
import time                                            # Ù„Ù‚ÙŠØ§Ø³ Ø²Ù…Ù† Ø§Ù„ØªÙ†ÙÙŠØ°
# --------------------------------------------
# âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
# --------------------------------------------
app = Flask(__name__)  # ğŸ§© Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ Flask Ø£Ø³Ø§Ø³ÙŠ

# ØªÙ‡ÙŠØ¦Ø© Ù…ÙØªØ§Ø­ Gemini API
genai.configure(api_key="AIzaSyBiVujRK7sBtyHN6ttxewS_2lMzvBEIk1A")

# ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø£Ø¬ÙˆØ¨Ø©
FAQ_PATH = os.path.join(os.path.dirname(__file__), "faq.json")

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Embeddings Ø®ÙÙŠÙ ÙˆØ³Ø±ÙŠØ¹ (ÙŠØ­ÙˆÙ‘Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ø¹Ø¯Ø¯ÙŠ)
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# --------------------------------------------
# âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ®ØµÙŠØµ
# --------------------------------------------
TOP_K = 5                   # Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø±Ø§Ø¯ Ø¬Ù„Ø¨Ù‡Ø§ Ù…Ù† Ø§Ù„Ø¨Ø­Ø«
EMB_WEIGHT = 0.7            # ÙˆØ²Ù† ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù€ Embeddings
TOKEN_WEIGHT = 0.3          # ÙˆØ²Ù† ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
COMBINED_THRESHOLD = 0.60   # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ´Ø§Ø¨Ù‡ Ù„Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø©

# --------------------------------------------
# ğŸš« Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© (Stopwords)
# --------------------------------------------
ARABIC_STOPWORDS = {
    "ÙÙŠ", "Ù…Ù†", "Ù…Ø§", "Ù‡ÙŠ", "Ù…Ø§Ù‡ÙŠ", "Ù…Ø§ Ù‡ÙŠ", "Ù„Ù…", "Ø¹Ù†", "Ø¹Ù„Ù‰", "Ùˆ", "Ø§Ùˆ", "Ø£Ùˆ",
    "Ù‡Ù„", "ÙƒÙŠÙ", "Ø£ÙŠÙ†", "ÙƒÙ…", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙƒÙˆÙ†", "ÙŠÙƒÙˆÙ†", "Ù‡Ùˆ", "Ù‡ÙŠ", "Ø¥Ù„Ù‰", "Ø¨"
}

# --------------------------------------------
# ğŸ§  Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„
# --------------------------------------------
questions, answers, token_sets = [], [], []    # Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§ØªØŒ ÙˆÙƒÙ„Ù…Ø§Øª ÙƒÙ„ Ø³Ø¤Ø§Ù„
nn_model = NearestNeighbors(n_neighbors=1, metric="cosine")  # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙƒÙˆÙ†ÙŠ

# --------------------------------------------
# ğŸ§¹ Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
# --------------------------------------------

def remove_diacritics(text):
    return re.sub(r'[\u0610-\u061A\u064B-\u065F\u06D6-\u06ED]', '', text)

#  ØªÙˆØ­ÙŠØ¯ Ø´ÙƒÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ (ØªØµØºÙŠØ±ØŒ Ø¥Ø²Ø§Ù„Ø© Ø±Ù…ÙˆØ²ØŒ Ø¥Ù„Ø®)
def normalize_ar(text):
    t = text.lower()
    t = remove_diacritics(t)
    t = re.sub(r'[^\u0600-\u06FF\s]', ' ', t)  # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ ÙÙ‚Ø· Ø¨Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    return re.sub(r'\s+', ' ', t).strip()

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª (Tokens) Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
def tokens_from_text(text):
    t = normalize_ar(text)
    return [w for w in t.split() if w and w not in ARABIC_STOPWORDS]

# Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©
def token_overlap_score(q_tokens, c_tokens):
    if not c_tokens:
        return 0.0
    return len(set(q_tokens) & set(c_tokens)) / max(len(c_tokens), 1)

# --------------------------------------------
# ğŸ’¾ ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# --------------------------------------------
def load_faq_data():
    if not os.path.exists(FAQ_PATH):
        return []
    try:
        with open(FAQ_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return []
#  Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Embeddings Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹
def build_index_from_memory():
    global nn_model
    if not questions:
        return
    embeddings = embedder.encode(questions, show_progress_bar=False)
    nn_model = NearestNeighbors(n_neighbors=min(len(questions), TOP_K), metric="cosine")
    nn_model.fit(embeddings)
#  ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³
def initialize_memory():
    global questions, answers, token_sets
    data = load_faq_data()
    
    # Reset lists
    questions.clear()
    answers.clear()
    token_sets.clear()
    
    # Extract questions and answers from nested structure
    for topic in data:
        for qa in topic.get("questions", []):
            question = qa.get("question", "")
            answer_list = qa.get("answers", [])
            
            if question and answer_list:
                questions.append(question)
                # Join multiple answers with newline if there are multiple
                answers.append("\n".join(answer_list))
                token_sets.append(tokens_from_text(question))
    
    if questions:
        build_index_from_memory()
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(questions)} Ø³Ø¤Ø§Ù„ ÙˆØ¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø³Ø¦Ù„Ø© Ù…Ø­ÙÙˆØ¸Ø© Ø¨Ø¹Ø¯.")

initialize_memory()

# --------------------------------------------
#  âœï¸ Ø­ÙØ¸ Ø£Ùˆ ØªØ­Ø¯ÙŠØ« Ø³Ø¤Ø§Ù„/Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# --------------------------------------------
def save_or_update_qa(question, answer):
    data = load_faq_data()
    q_tokens = tokens_from_text(question)
    found_idx = None
    found_topic = None

  # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø³Ø¤Ø§Ù„ Ù…Ø´Ø§Ø¨Ù‡ Ù„ØªØ­Ø¯ÙŠØ«Ù‡ Ø¨Ø¯Ù„ Ù…Ù† Ø¥Ø¶Ø§ÙØ© Ø¬Ø¯ÙŠØ¯
    for topic in data:
        for qa in topic.get("questions", []):
            if token_overlap_score(q_tokens, tokens_from_text(qa["question"])) >= 0.6:
                found_topic = topic
                found_idx = data.index(topic)
                break
        if found_topic:
            break

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù†ØµØ§Ù‹
    answer_list = answer.split("\n") if isinstance(answer, str) else answer

    if found_topic:
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
        for qa in found_topic["questions"]:
            if token_overlap_score(q_tokens, tokens_from_text(qa["question"])) >= 0.6:
                qa["answers"] = answer_list
                break
    else:
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÙŠØ¯
        new_topic = {
            "topic": extract_topic(question), 
            "questions": [{
                "question": question,
                "answers": answer_list
            }]
        }
        data.append(new_topic)

    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù„Ù
    with open(FAQ_PATH, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    initialize_memory()

def extract_topic(question):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù…Ù† Ø§Ù„Ø³Ø¤Ø§Ù„"""
    # Ø¥Ø²Ø§Ù„Ø© ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
    topic = question.replace("Ù…Ø§ Ù‡ÙŠ", "").replace("Ù…Ø§ Ù‡Ùˆ", "").replace("ØŸ", "").strip()
    # Ø£Ø®Ø° Ø£ÙˆÙ„ 3 ÙƒÙ„Ù…Ø§Øª ÙƒÙ…ÙˆØ¶ÙˆØ¹
    words = topic.split()[:3]
    return " ".join(words)

def filter_answers_by_query(user_text, data, min_token_len=3):
    """
    ÙÙ„ØªØ±Ø© Ø¹Ø§Ù…Ø©: Ø¥Ø°Ø§ Ø³Ø£Ù„Øª Ø¹Ù† Ø´ÙŠØ¡ Ù…Ø­Ø¯Ø¯ØŒ Ù†Ø¹ÙŠØ¯ ÙÙ‚Ø· Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
    Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ (Ø¨Ø¯ÙˆÙ† Ø¥Ø¶Ø§ÙØ© Ø¥Ø¬Ø§Ø¨Ø§Øª ØºÙŠØ± Ù…ØªØ¹Ù„Ù‚Ø©).
    Ø§Ù„Ø±Ø¯ Ø¨Ù†ÙØ³ Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ø±Ø³Ù„Ù‡
    - user_text: Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ Ø£Ùˆ Ø§Ù„Ù…ØªØ±Ø¬Ù… Ù„Ù„Ø¨Ø­Ø«.
    - data: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù€ FAQ.
    """
    tokens = [t for t in tokens_from_text(user_text) if len(t) >= min_token_len]
    if not tokens:
        return None

    matches = []
    for topic in data:
        for qa in topic.get("questions", []):
            # Ù†Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ ÙƒÙ„ Ø¥Ø¬Ø§Ø¨Ø© ÙˆÙ†Ø¶ÙŠÙÙ‡Ø§ Ø¥Ø°Ø§ ÙˆÙØ¬Ø¯ Ø£ÙŠ ØªÙˆÙƒÙ†
            for ans in qa.get("answers", []):
                norm_ans = normalize_ar(ans)
                for tok in tokens:
                    if tok in norm_ans:
                        matches.append(ans)
                        break

            # ÙƒÙ…Ø§Ù† Ù†Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ù†Øµ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø®Ø²Ù† (ÙÙŠ Ø­Ø§Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù‚ØµÙŠØ±Ø© ÙˆØºÙŠØ± Ù…ÙØµÙ‘Ù„Ø©)
            norm_q = normalize_ar(qa.get("question", ""))
            for tok in tokens:
                if tok in norm_q:
                    # Ù†Ø¶ÙŠÙ ÙƒÙ„ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„
                    matches.extend(qa.get("answers", []))
                    break

    if matches:
        # Ø¥Ø±Ø¬Ø§Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª ÙØ±ÙŠØ¯Ø© Ù…Ø±ØªØ¨Ø© ÙƒÙ…Ø§ ÙˆÙØ¬Ø¯Øª
        return "\n".join(dict.fromkeys(matches))
    return None

# --------------------------------------------
# ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„ØªØ±Ø¬Ù…Ø©
# --------------------------------------------

def get_best_answer(user_input):
    original_text = user_input
    answer = ""

    # ---------------------------
    # ğŸ”¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„ØºØ© + Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ø­ÙŠØ¨ Ø¨Ù†ÙØ³ Ø§Ù„Ù„ØºØ©
    # ---------------------------
    t1 = time.time()
    model = genai.GenerativeModel("models/gemini-2.5-pro")
    try:
        resp = model.generate_content(
            f"""
            You are a multilingual assistant.
            Step 1ï¸âƒ£: Detect the language of this text.
            Step 2ï¸âƒ£: If the text is only a greeting (like hello, hi, Ù…Ø±Ø­Ø¨Ø§, hola, bonjour, etc.), 
            then reply in the same detected language with a warm greeting message followed by "How can I help you today?" in that language.
            Step 3ï¸âƒ£: Otherwise, just reply with the language name only (Arabic, English, French, etc.).
            
            User text:
            {user_input}
            """
        )

        detected_text = resp.text.strip()

        # âœ… Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¬Ù…Ù„Ø© ØªØ±Ø­ÙŠØ¨ ÙƒØ§Ù…Ù„Ø© (ÙˆÙ„ÙŠØ³ ÙÙ‚Ø· Ø§Ø³Ù… Ù„ØºØ©)
        if any(word in detected_text.lower() for word in ["help", "Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ", "aider", "ayudar", "aiutare"]):
            return detected_text  # Ø§Ù„Ø±Ø¯ Ø§Ù„ØªØ±Ø­ÙŠØ¨ÙŠ Ø§Ù„Ø¬Ø§Ù‡Ø² Ù…Ù† Gemini

        # âœ… ÙˆØ¥Ù„Ø§ ÙÙ‡ÙŠ Ù…Ø¬Ø±Ø¯ Ø§Ø³Ù… Ù„ØºØ© (Ø²ÙŠ "Arabic", "English" ...)
        detected_lang = detected_text.split()[0].capitalize()

    except Exception as e:
        print("âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„ØºØ© Ø£Ùˆ Ø§Ù„Ø±Ø¯ Ø§Ù„ØªØ±Ø­ÙŠØ¨ÙŠ:", e)
        detected_lang = "Arabic"

    # ---------------------------
    # ğŸ”¹ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
    # ---------------------------
    t2 = time.time()
    translated_for_search = user_input
    if detected_lang.lower() != "arabic":
        try:
            model = genai.GenerativeModel("models/gemini-2.5-pro")
            prompt = (
                "Translate the following text to Arabic. "
                "Reply ONLY with the translated Arabic text, no explanations, no notes, no markdown:\n\n"
                f"{user_input}"
            )
            resp = model.generate_content(prompt)
            translated_for_search = re.sub(
                r"(?i)(here is the translation|translation|of course|sure|the answer is|:)",
                "",
                resp.text.strip(),
            ).strip()
        except Exception as e:
            print("âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ±Ø¬Ù…Ø©:", e)

    # ---------------------------
    # ğŸ™ï¸ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø­ÙŠØ§Ø¡ ÙˆØ§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
    # ---------------------------
    data = load_faq_data()
    normalized_q = normalize_ar(translated_for_search)

    # ---------------------------
    # âœ… ÙÙ„ØªØ±Ø© Ø¹Ø§Ù…Ø©: Ù„Ùˆ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ°ÙƒØ± Ø´ÙŠØ¡ Ù…Ø­Ø¯Ø¯ØŒ Ø£Ø¹Ø¯ ÙÙ‚Ø· Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ÙØ³ Ø§Ù„ÙƒÙ„Ù…Ø©/Ø§Ù„Ù…ÙÙ‡ÙˆÙ…
    # ---------------------------
    filtered_answers = filter_answers_by_query(translated_for_search, data)
    if filtered_answers:
        if detected_lang.lower() != "arabic":
            try:
                model = genai.GenerativeModel("models/gemini-2.5-pro")
                prompt = (
                    f"Translate the following Arabic text to {detected_lang}. "
                    "Reply ONLY with the translated text, no explanations:\n\n"
                    f"{filtered_answers}"
                )
                resp = model.generate_content(prompt)
                clean_text = re.sub(
                    r"(?i)(here is the translation|of course|translation|sure|the answer is|Here is the English|:)",
                    "",
                    resp.text.strip()
                ).strip()
                return clean_text
            except Exception as e:
                print("âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…ÙÙ„ØªØ±Ø©:", e)
                return filtered_answers
        return filtered_answers

    if "Ø­ÙŠ" in normalized_q or "Ø§Ø­ÙŠØ§Ø¡" in normalized_q or "Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†" in normalized_q:
        for topic in data:
            if normalize_ar(topic.get("topic", "")) == "Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†":
                # Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø£Ø³Ø¦Ù„Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„ØªÙˆØ¨ÙŠÙƒ
                questions_list = topic.get("questions", [])
                if not questions_list:
                    break

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¯Ù† Ù…Ù† Ø£ÙˆÙ„ Ø¥Ø¬Ø§Ø¨Ø©
                cities = questions_list[0].get("answers", [])
                city_text = " ".join(cities)
                cities_cleaned = [
                    c.strip().replace("ØŒ", "").replace(".", "")
                    for c in city_text.split()
                    if len(c.strip()) > 1
                ]

                # ğŸ”¹ Ù†Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ù…Ø¯ÙŠÙ†Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„
                for city in cities_cleaned:
                    if normalize_ar(city) in normalized_q:
                        # Ù†Ù„Ø§Ù‚ÙŠ Ø§Ù„ØªÙˆØ¨ÙŠÙƒ Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©
                        for sub_topic in data:
                            if normalize_ar(sub_topic.get("topic", "")) == f"Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† {normalize_ar(city)}":
                                areas = []
                                for q in sub_topic.get("questions", []):
                                    for ans in q.get("answers", []):
                                        areas.extend(ans.replace("ØŒ", ",").split(","))
                                areas = [a.strip() for a in areas if a.strip()]

                                # ğŸ”¹ Ù†Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø­ÙŠ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„
                                for area in areas:
                                    if normalize_ar(area) in normalized_q:
                                        return f"Ù†Ø¹Ù…ØŒ Ø­ÙŠ {area} Ù…ÙˆØ¬ÙˆØ¯ âœ…"

                                # ğŸ”¹ Ù„Ùˆ Ø§Ù„Ø­ÙŠ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
                                return (
                                    f"Ø§Ù„Ø­ÙŠ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ {city} âŒ\n"
                                    f"Ù‡Ù„ ØªØ±ØºØ¨ Ø£Ù† Ø£Ø¸Ù‡Ø± Ù„Ùƒ Ø§Ù„Ø£Ø­ÙŠØ§Ø¡ Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ {city}ØŸ\n\n"
                                    "Ø§ÙƒØªØ¨ Ø§Ø³Ù… Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø¢Ù† ÙˆØ³Ø£Ø¹Ø±Ø¶Ù‡Ø§ Ù„Ùƒ ğŸ‘‡"
                                )

                # ğŸ”¹ Ù„Ùˆ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø­ÙŠ Ø¨Ø¯ÙˆÙ† Ø°ÙƒØ± Ù…Ø¯ÙŠÙ†Ø©
                all_areas = []
                for sub_topic in data:
                    if normalize_ar(sub_topic.get("topic", "")).startswith("Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†"):
                        for q in sub_topic.get("questions", []):
                            for ans in q.get("answers", []):
                                all_areas.extend(ans.replace("ØŒ", ",").split(","))
                all_areas = [a.strip() for a in all_areas if a.strip()]

                for area in all_areas:
                    if normalize_ar(area) in normalized_q:
                        return f"Ù†Ø¹Ù…ØŒ Ø­ÙŠ {area} Ù…ÙˆØ¬ÙˆØ¯ âœ…"

                # ğŸ”¹ Ù„Ùˆ Ø§Ù„Ø­ÙŠ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ØªÙ…Ø§Ù…Ù‹Ø§
                return (
                    "Ø§Ù„Ø­ÙŠ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ âŒ\n"
                    "Ù…Ù† ÙØ¶Ù„Ùƒ Ø§Ø®ØªØ± Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø£Ø­ÙŠØ§Ø¡ Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠÙ‡Ø§ ğŸ‘‡\n\n"
                    "Ø§Ù„Ù…Ø¯Ù† Ø§Ù„Ù…ØªØ§Ø­Ø©: Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø¬Ø¯Ø©ØŒ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ù…Ù†ÙˆØ±Ø©"
                )

    # ---------------------------
    # ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
    # ---------------------------
    t3 = time.time()
    if not questions:
        answer = "Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ø­Ø§Ù„ÙŠØ§Ù‹."
    else:
        keywords = [w.strip("ØŸ,.ØŒ") for w in translated_for_search.split() if len(w) > 3]
        q_vec = embedder.encode([translated_for_search])
        k = min(TOP_K, len(questions))
        dist, idxs = nn_model.kneighbors(q_vec, n_neighbors=k)

        candidates = []
        for rank, idx in enumerate(idxs[0]):
            emb_sim = 1 - dist[0][rank]
            keyword_match = any(
                keyword in questions[idx].lower() or keyword in answers[idx].lower()
                for keyword in keywords
            )
            if keyword_match and emb_sim >= COMBINED_THRESHOLD:
                candidates.append((emb_sim, answers[idx]))

        answer = candidates[0][1] if candidates else "Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ø­Ø§Ù„ÙŠØ§Ù‹."

    # ---------------------------
    # ğŸ”¹ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¥Ù„Ù‰ Ù„ØºØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    # ---------------------------
    t4 = time.time()
    final_answer = answer
    if detected_lang.lower() != "arabic":
        try:
            model = genai.GenerativeModel("models/gemini-2.5-pro")
            prompt = (
                f"Translate the following Arabic text to {detected_lang}. "
                "Reply ONLY with the translated text, no explanations:\n\n"
                f"{answer}"
            )
            resp = model.generate_content(prompt)
            clean_text = re.sub(
                r"(?i)(here is the translation|of course|translation|sure|the answer is|Here is the English|:)",
                "",
                resp.text.strip()
            ).strip()
            final_answer = clean_text
        except Exception as e:
            print("âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:", e)

    # ---------------------------
    # ğŸ’¾ Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    # ---------------------------
    t5 = time.time()
    try:
        save_or_update_qa(translated_for_search, answer)
    except Exception as e:
        print("âš ï¸ ÙØ´Ù„ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­ÙØ¸:", e)
    return final_answer
# --------------------------------------------
# ğŸ“¤ Ø±ÙØ¹ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø£Ø¬ÙˆØ¨Ø© Ù„Ø¨ÙˆØ³ØªÙ…Ø§Ù† (upload_faq)
# --------------------------------------------
FAQ_PATH = "faq_data.json"

def initialize_memory():
    # Ù‡Ù†Ø§ ØªÙ‚Ø¯Ø±Ù ØªØ­Ø·ÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù„ÙŠ Ø¨ÙŠØ¨Ù†ÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ Ø£Ùˆ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    print("âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ù†Ø¬Ø§Ø­.")

@app.route("/upload_faq", methods=["GET", "POST"])
def upload_faq():
    # âœ… Ù„Ùˆ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙØªØ­ Ø§Ù„Ø±Ø§Ø¨Ø· ÙÙŠ Ø§Ù„Ù…ØªØµÙØ­ (GET)
    if request.method == "GET":
        if os.path.exists(FAQ_PATH):
            with open(FAQ_PATH, "r", encoding="utf-8") as f:
                data = json.load(f)
            # Ù†Ø±Ø¬Ø¹ JSON Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø³Ù‚
            return jsonify(data)
        else:
            return jsonify({"message": "âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯."}), 404

    # âœ… Ù„Ùˆ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø±ÙØ¹ Ø¨ÙŠØ§Ù†Ø§Øª (POST)
    try:
        data = request.json
        if not data:
            return jsonify({"error": "Ù„Ù… ÙŠØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª."}), 400
        if not isinstance(data, list):
            return jsonify({"error": "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù‚Ø§Ø¦Ù…Ø© (list) Ù…Ù† Ø§Ù„Ø¹Ù†Ø§ØµØ±."}), 400

        with open(FAQ_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        initialize_memory()
        return jsonify({"message": f"âœ… ØªÙ… Ø±ÙØ¹ ÙˆØ­ÙØ¸ {len(data)} Ù…ÙˆØ¶ÙˆØ¹ Ø¨Ù†Ø¬Ø§Ø­."}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# --------------------------------------------
# ğŸ’¬ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© (API)
# --------------------------------------------
@app.route("/chat", methods=["POST"])
def chat():
    user_input = request.json.get("message", "")
    session_id = request.json.get("session_id", "default")
    reply = get_best_answer(user_input)
    pretty_log_question_answer(user_input, reply)
    return jsonify({"reply": reply})
def pretty_log_question_answer(user_input, reply):
    """Ø·Ø¨Ø§Ø¹Ø© Ù…Ù†Ø³Ù‚Ø© Ù„Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„ØªÙŠØ±Ù…ÙŠÙ†Ø§Ù„"""
    from bidi.algorithm import get_display
    import datetime, sys

    # ØªØµØ­ÙŠØ­ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ + ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„ØªØ±Ù…ÙŠØ² UTF-8
    sys.stdout.reconfigure(encoding="utf-8")
    q_disp = get_display(user_input)
    a_disp = get_display(reply)
    now = datetime.datetime.now().strftime("%H:%M:%S")

    # Ø§Ù„Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ù†ÙØ³ Ø´ÙƒÙ„ LogØ§Øª Flutter
    print("\n" + "=" * 60)
    print(f"ğŸ•’ [{now}]")
    print(f"ğŸ“© [USER QUESTION]: {q_disp}")
    print(f"ğŸ¤– [BOT ANSWER]: {a_disp}")
    print("=" * 60 + "\n")

@app.route('/favicon.ico')
def favicon():
    return send_from_directory('static', 'favicon.ico')
# --------------------------------------------
# ğŸš€ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù…
# --------------------------------------------
if __name__ == "__main__":
    app.run(port=5000, debug=True)

# ============================================
# ğŸ¤– Chatbot AI â€” Ù†Ø¸Ø§Ù… Ø£Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø© Ø°ÙƒÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Embeddings + Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
# ============================================

from flask import Flask, request, jsonify
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import NearestNeighbors
import google.generativeai as genai
import json, os, re, requests
from bs4 import BeautifulSoup

# --------------------------------------------
# âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
# --------------------------------------------
app = Flask(__name__)

# Ù…ÙØªØ§Ø­ Gemini API
genai.configure(api_key="AIzaSyDyHN4DInZrAHrUHbObZchZGS21VEEKBoU")

# Ù…Ø³Ø§Ø± Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
FAQ_PATH = os.path.join(os.path.dirname(__file__), "faq.json")

# Ù†Ù…ÙˆØ°Ø¬ Embeddings Ø®ÙÙŠÙ ÙˆØ³Ø±ÙŠØ¹
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# --------------------------------------------
# âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ®ØµÙŠØµ
# --------------------------------------------
TOP_K = 5
EMB_WEIGHT = 0.7
TOKEN_WEIGHT = 0.3
COMBINED_THRESHOLD = 0.60

# --------------------------------------------
# ğŸš« Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© (Stopwords)
# --------------------------------------------
ARABIC_STOPWORDS = {
    "ÙÙŠ", "Ù…Ù†", "Ù…Ø§", "Ù‡ÙŠ", "Ù…Ø§Ù‡ÙŠ", "Ù…Ø§ Ù‡ÙŠ", "Ù„Ù…", "Ø¹Ù†", "Ø¹Ù„Ù‰", "Ùˆ", "Ø§Ùˆ", "Ø£Ùˆ",
    "Ù‡Ù„", "ÙƒÙŠÙ", "Ø£ÙŠÙ†", "ÙƒÙ…", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙƒÙˆÙ†", "ÙŠÙƒÙˆÙ†", "Ù‡Ùˆ", "Ù‡ÙŠ", "Ø¥Ù„Ù‰", "Ø¨"
}

# --------------------------------------------
# ğŸ§  Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„
# --------------------------------------------
questions, answers, token_sets = [], [], []
nn_model = NearestNeighbors(n_neighbors=1, metric="cosine")

# --------------------------------------------
# ğŸ§¹ Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
# --------------------------------------------

def remove_diacritics(text):
    return re.sub(r'[\u0610-\u061A\u064B-\u065F\u06D6-\u06ED]', '', text)

def normalize_ar(text):
    t = text.lower()
    t = remove_diacritics(t)
    t = re.sub(r'[^\u0600-\u06FF\s]', ' ', t)
    return re.sub(r'\s+', ' ', t).strip()

def tokens_from_text(text):
    t = normalize_ar(text)
    return [w for w in t.split() if w and w not in ARABIC_STOPWORDS]

def token_overlap_score(q_tokens, c_tokens):
    if not c_tokens:
        return 0.0
    return len(set(q_tokens) & set(c_tokens)) / max(len(c_tokens), 1)

# --------------------------------------------
# ğŸ’¾ ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# --------------------------------------------
def load_faq_data():
    if not os.path.exists(FAQ_PATH):
        return []
    try:
        with open(FAQ_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return []

def build_index_from_memory():
    global nn_model
    if not questions:
        return
    embeddings = embedder.encode(questions, show_progress_bar=False)
    nn_model = NearestNeighbors(n_neighbors=min(len(questions), TOP_K), metric="cosine")
    nn_model.fit(embeddings)

def initialize_memory():
    global questions, answers, token_sets
    data = load_faq_data()
    
    # Reset lists
    questions.clear()
    answers.clear()
    token_sets.clear()
    
    # Extract questions and answers from nested structure
    for topic in data:
        for qa in topic.get("questions", []):
            question = qa.get("question", "")
            answer_list = qa.get("answers", [])
            
            if question and answer_list:
                questions.append(question)
                # Join multiple answers with newline if there are multiple
                answers.append("\n".join(answer_list))
                token_sets.append(tokens_from_text(question))
    
    if questions:
        build_index_from_memory()
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(questions)} Ø³Ø¤Ø§Ù„ ÙˆØ¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø³Ø¦Ù„Ø© Ù…Ø­ÙÙˆØ¸Ø© Ø¨Ø¹Ø¯.")

initialize_memory()

# --------------------------------------------
# âœï¸ Ø­ÙØ¸ Ø£Ùˆ ØªØ­Ø¯ÙŠØ« Ø³Ø¤Ø§Ù„/Ø¥Ø¬Ø§Ø¨Ø©
# --------------------------------------------
def save_or_update_qa(question, answer):
    data = load_faq_data()
    q_tokens = tokens_from_text(question)
    found_idx = None
    found_topic = None

    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø³Ø¤Ø§Ù„ Ù…Ø´Ø§Ø¨Ù‡
    for topic in data:
        for qa in topic.get("questions", []):
            if token_overlap_score(q_tokens, tokens_from_text(qa["question"])) >= 0.6:
                found_topic = topic
                found_idx = data.index(topic)
                break
        if found_topic:
            break

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù†ØµØ§Ù‹
    answer_list = answer.split("\n") if isinstance(answer, str) else answer

    if found_topic:
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
        for qa in found_topic["questions"]:
            if token_overlap_score(q_tokens, tokens_from_text(qa["question"])) >= 0.6:
                qa["answers"] = answer_list
                break
    else:
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÙŠØ¯
        new_topic = {
            "topic": extract_topic(question),  # Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ø³Ù†Ø¶ÙŠÙÙ‡Ø§
            "questions": [{
                "question": question,
                "answers": answer_list
            }]
        }
        data.append(new_topic)

    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù„Ù
    with open(FAQ_PATH, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    initialize_memory()

def extract_topic(question):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù…Ù† Ø§Ù„Ø³Ø¤Ø§Ù„"""
    # Ø¥Ø²Ø§Ù„Ø© ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
    topic = question.replace("Ù…Ø§ Ù‡ÙŠ", "").replace("Ù…Ø§ Ù‡Ùˆ", "").replace("ØŸ", "").strip()
    # Ø£Ø®Ø° Ø£ÙˆÙ„ 3 ÙƒÙ„Ù…Ø§Øª ÙƒÙ…ÙˆØ¶ÙˆØ¹
    words = topic.split()[:3]
    return " ".join(words)

# --------------------------------------------
# ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„ØªØ±Ø¬Ù…Ø©
# --------------------------------------------
def get_best_answer(user_input):
    """
    Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙØ¶Ù„ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©
    """
    original_text = user_input
    answer = ""

    # ---------------------------
    # ğŸ”¹ ØªØ­Ø¯ÙŠØ¯ Ù„ØºØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©
    # ---------------------------
    detected_lang = "Arabic"
    try:
        model = genai.GenerativeModel("models/gemini-2.5-pro")
        resp = model.generate_content(
            f"Detect the language of this text only. Reply with one word like: Arabic, English, French, etc.\n\n{user_input}"
        )
        detected_lang = resp.text.strip().capitalize()
    except Exception as e:
        print("âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„ØºØ©:", e)

    # ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
    translated_for_search = user_input
    if detected_lang.lower() != "arabic":
        try:
            model = genai.GenerativeModel("models/gemini-2.5-pro")
            prompt = (
                "Translate the following text to Arabic. "
                "Reply ONLY with the translated Arabic text, no explanations, no notes, no markdown:\n\n"
                f"{user_input}"
            )
            resp = model.generate_content(prompt)
            translated_for_search = re.sub(
                r"(?i)(here is the translation|translation|of course|sure|the answer is|:)",
                "",
                resp.text.strip(),
            ).strip()

        except Exception as e:
            print("âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ±Ø¬Ù…Ø©:", e)

    # ---------------------------
    # ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
    # ---------------------------
    if not questions:
        answer = "Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ø­Ø§Ù„ÙŠØ§Ù‹."
    else:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        keywords = [w.strip("ØŸ,.ØŒ") for w in translated_for_search.split() if len(w) > 3]
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Embeddings
        q_vec = embedder.encode([translated_for_search])
        k = min(TOP_K, len(questions))
        dist, idxs = nn_model.kneighbors(q_vec, n_neighbors=k)

        candidates = []
        for rank, idx in enumerate(idxs[0]):
            emb_sim = 1 - dist[0][rank]
            # ÙØ­Øµ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            keyword_match = False
            for keyword in keywords:
                if (keyword in questions[idx].lower() or 
                    keyword in answers[idx].lower()):
                    keyword_match = True
                    break
            
            if keyword_match and emb_sim >= COMBINED_THRESHOLD:
                candidates.append((emb_sim, answers[idx]))

        if candidates:
            # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
            candidates.sort(reverse=True)
            answer = candidates[0][1]
        else:
            answer = "Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ø­Ø§Ù„ÙŠØ§Ù‹."

    # ---------------------------
    # ğŸ”¹ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¥Ù„Ù‰ Ù„ØºØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    # ---------------------------
    final_answer = answer
    if detected_lang.lower() != "arabic":
        try:
            model = genai.GenerativeModel("models/gemini-2.5-pro")
            prompt = (
                 "Translate the following text to Arabic. "
                 "Reply ONLY with the translated Arabic text, no explanations, no notes, no markdown:\n\n"
                f"Translate the following Arabic text to {detected_lang}:\n\n{answer}"
            )
            resp = model.generate_content(prompt)
            clean_text = re.sub(
            r"(?i)(here is the translation|of course|translation|sure|the answer is|Here is the English|:)",
            "",
            resp.text.strip()
            ).strip()
            final_answer = clean_text

        except Exception as e:
            print("âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:", e)

    # Ø­ÙØ¸ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©
    try:
        save_or_update_qa(translated_for_search, answer)
    except Exception as e:
        print("âš ï¸ ÙØ´Ù„ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­ÙØ¸:", e)

    return final_answer

# --------------------------------------------
# ğŸ’¬ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© (API)
# --------------------------------------------
@app.route("/chat", methods=["POST"])
def chat():
    user_input = request.json.get("message", "")
    session_id = request.json.get("session_id", "default")

    reply = get_best_answer(user_input)
    return jsonify({"reply": reply})

# --------------------------------------------
# ğŸš€ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù…
# --------------------------------------------
if __name__ == "__main__":
    app.run(port=5000, debug=True)

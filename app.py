# ============================================
# ğŸ¤– chatbot AI - FAQ System using Embeddings + Keywords
# ============================================

from flask import Flask, request, jsonify
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import NearestNeighbors
import google.generativeai as genai
import json, os, re, requests
from bs4 import BeautifulSoup

# --------------------------------------------
#  Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø©
# --------------------------------------------
app = Flask(__name__)

# Ù…ÙØªØ§Ø­ ÙˆØ§Ø¬Ù‡Ø© Gemini
genai.configure(api_key="AIzaSyBEeidGnK_uyf9ikJWW9elsAgDdz8t09oA")

# Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
FAQ_PATH = os.path.join(os.path.dirname(__file__), "faq.json")

# Ù†Ù…ÙˆØ°Ø¬ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Embeddings (Ù†Ù…ÙˆØ°Ø¬ Ø®ÙÙŠÙ ÙˆØ³Ø±ÙŠØ¹)
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# --------------------------------------------
#  Ù…Ø¹Ù„Ù…Ø§Øª ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§
# --------------------------------------------
TOP_K = 5                # ÙƒÙ… Ù†ØªÙŠØ¬Ø© ÙŠØ¨Ø­Ø« Ø¹Ù†Ù‡Ø§ ÙÙŠ ÙƒÙ„ Ù…Ø±Ø©
EMB_WEIGHT = 0.7         # ÙˆØ²Ù† ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù€ Embeddings
TOKEN_WEIGHT = 0.3       # ÙˆØ²Ù† ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
COMBINED_THRESHOLD = 0.60
EMB_MIN_ACCEPT = 0.62
TOKEN_MIN_ACCEPT = 0.15

# --------------------------------------------
#  ÙƒÙ„Ù…Ø§Øª Ø´Ø§Ø¦Ø¹Ø© (Stopwords) ÙŠØªÙ… ØªØ¬Ø§Ù‡Ù„Ù‡Ø§ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«
# --------------------------------------------
ARABIC_STOPWORDS = {
    "ÙÙŠ", "Ù…Ù†", "Ù…Ø§", "Ù‡ÙŠ", "Ù…Ø§Ù‡ÙŠ", "Ù…Ø§ Ù‡ÙŠ", "Ù„Ù…", "Ø¹Ù†", "Ø¹Ù„Ù‰", "Ùˆ", "Ø§Ùˆ", "Ø£Ùˆ",
    "Ù‡Ù„", "ÙƒÙŠÙ", "Ø£ÙŠÙ†", "ÙƒÙ…", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙƒÙˆÙ†", "ÙŠÙƒÙˆÙ†", "Ù‡Ùˆ", "Ù‡ÙŠ", "Ø¥Ù„Ù‰", "Ø¨"
}

# --------------------------------------------
#  Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙÙŠ runtime
# --------------------------------------------
questions = []
answers = []
token_sets = []
nn_model = NearestNeighbors(n_neighbors=1, metric="cosine")
last_added_question = None

# --------------------------------------------
#  Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ
# --------------------------------------------

def remove_diacritics(text: str) -> str:
    """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
    return re.sub(r'[\u0610-\u061A\u064B-\u065F\u06D6-\u06ED]', '', text)

def normalize_ar(text: str) -> str:
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ù„Ø­Ø±ÙˆÙ Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø±Ù…ÙˆØ² Ø£Ùˆ ØªØ´ÙƒÙŠÙ„"""
    t = text.lower()
    t = remove_diacritics(t)
    t = re.sub(r'[^\u0600-\u06FF\s]', ' ', t)
    return re.sub(r'\s+', ' ', t).strip()

def tokens_from_text(text: str):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‡Ø§Ù…Ø© ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† stopwords)"""
    t = normalize_ar(text)
    return [w for w in t.split() if w and w not in ARABIC_STOPWORDS]

def token_overlap_score(query_tokens, cand_tokens):
    """Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© ØªØ¯Ø§Ø®Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¨ÙŠÙ† Ø³Ø¤Ø§Ù„ÙŠÙ†"""
    if not cand_tokens:
        return 0.0
    qset, cset = set(query_tokens), set(cand_tokens)
    return len(qset.intersection(cset)) / max(len(cset), 1)

# --------------------------------------------
#  ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# --------------------------------------------

def load_faq_data():
    """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù faq.json"""
    if not os.path.exists(FAQ_PATH):
        return []
    try:
        with open(FAQ_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return []

def build_index_from_memory():
    """Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨Ø­Ø« (index)"""
    global nn_model
    if not questions:
        return
    embeddings = embedder.encode(questions, show_progress_bar=False)
    k = min(len(questions), TOP_K)
    nn_model = NearestNeighbors(n_neighbors=k, metric="cosine")
    nn_model.fit(embeddings)

def initialize_memory():
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„"""
    global questions, answers, token_sets
    data = load_faq_data()
    questions = [d["question"] for d in data]
    answers = [d["answer"] for d in data]
    token_sets = [tokens_from_text(q) for q in questions]
    if questions:
        build_index_from_memory()
        print(f" ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(questions)} Ø³Ø¤Ø§Ù„ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        print(" ØªÙ… Ø¨Ù†Ø§Ø¡ Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© (Embeddings index) Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print(" Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø³Ø¦Ù„Ø© Ù…Ø­ÙÙˆØ¸Ø© Ø¨Ø¹Ø¯.")

initialize_memory()

# --------------------------------------------
#  ØªØ­Ø¯ÙŠØ« Ø£Ùˆ Ø¥Ø¶Ø§ÙØ© Ø³Ø¤Ø§Ù„ Ø¬Ø¯ÙŠØ¯
# --------------------------------------------

def save_or_update_qa(question, answer):
    """ØªØ­Ø¯ÙŠØ« Ø£Ùˆ Ø¥Ø¶Ø§ÙØ© Ø³Ø¤Ø§Ù„ Ø¬Ø¯ÙŠØ¯ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    global questions, answers, token_sets

    data = load_faq_data()
    q_toks = tokens_from_text(question)
    found_idx = None

    # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø³Ø¤Ø§Ù„ Ù…Ø´Ø§Ø¨Ù‡
    for i, q in enumerate(questions):
        if token_overlap_score(q_toks, token_sets[i]) >= 0.6:
            found_idx = i
            break

    # Ù„Ùˆ Ù…ÙˆØ¬ÙˆØ¯: Ø­Ø¯Ø« Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
    if found_idx is not None:
        old_q = questions[found_idx]
        for item in data:
            if item["question"].strip() == old_q.strip():
                item["answer"] = answer
                break
        answers[found_idx] = answer
    else:
        # Ù„Ùˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: Ø£Ø¶Ù Ø³Ø¤Ø§Ù„ Ø¬Ø¯ÙŠØ¯
        data.append({"question": question, "answer": answer})
        questions.append(question)
        answers.append(answer)
        token_sets.append(q_toks)

    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
    with open(FAQ_PATH, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    build_index_from_memory()

# --------------------------------------------
#  Web Scraping Ø¨Ø³ÙŠØ· (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
# --------------------------------------------

def get_answer_from_url(question):
    """Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø®Ø§Ø±Ø¬ÙŠ (ÙƒÙ…ØµØ¯Ø± Ø¨Ø¯ÙŠÙ„)"""
    url = "https://www.mueen.com.sa/ar/"
    try:
        resp = requests.get(url, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")

        for block in soup.select(".faq-item"):
            q = block.select_one(".faq-question")
            a = block.select_one(".faq-answer")
            if q and a:
                q_text, a_text = q.get_text(strip=True), a.get_text(strip=True)
                if token_overlap_score(tokens_from_text(question), tokens_from_text(q_text)) > 0:
                    return a_text
    except Exception as e:
        print("scraping error:", e)
    return None

# --------------------------------------------
#  Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙØ¶Ù„ Ø¥Ø¬Ø§Ø¨Ø© (core logic)
# --------------------------------------------

def get_best_answer(user_input):
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ù‚Ø±Ø¨ Ø¥Ø¬Ø§Ø¨Ø©"""
    global last_added_question

    user_toks = tokens_from_text(user_input)

    # Ù„Ùˆ Ù…Ø§ ÙÙŠØ´ Ø£Ø³Ø¦Ù„Ø© Ø¨Ø¹Ø¯
    if not questions:
        scraped = get_answer_from_url(user_input)
        if scraped:
            save_or_update_qa(user_input, scraped)
            return scraped
        save_or_update_qa(user_input, "Ø³ÙŠØªÙ… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹")
        return "Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ø­Ø§Ù„ÙŠØ§Ù‹ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø£Ù† ØªØ®Ø¨Ø±Ù†ÙŠ Ø¨Ø§Ù„Ø±Ø¯ Ø§Ù„ØµØ­ÙŠØ­."

    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù€ Embedding Ù„Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
    q_vec = embedder.encode([user_input])
    k = min(TOP_K, len(questions))
    dist, idxs = nn_model.kneighbors(q_vec, n_neighbors=k)

    best_idx, best_score = None, -1

    for rank, cand_idx in enumerate(idxs[0]):
        emb_sim = 1 - dist[0][rank]  # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¥Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡
        tok_overlap = token_overlap_score(user_toks, token_sets[cand_idx])
        combined = EMB_WEIGHT * emb_sim + TOKEN_WEIGHT * tok_overlap
        if combined > best_score:
            best_score = combined
            best_idx = cand_idx

    # Ù„Ùˆ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù‚ÙˆÙŠØ© Ø¨Ù…Ø§ ÙŠÙƒÙÙŠ
    if best_idx is not None and best_score >= COMBINED_THRESHOLD:
        answer = answers[best_idx]
        save_or_update_qa(user_input, answer)
        return answer

    # ØªØ¬Ø±Ø¨Ø© Web Scraping ÙƒÙ…ØµØ¯Ø± Ø¨Ø¯ÙŠÙ„
    scraped = get_answer_from_url(user_input)
    if scraped:
        save_or_update_qa(user_input, scraped)
        return scraped

    # Ù„Ùˆ Ù…ÙÙŠØ´ Ø­Ø§Ø¬Ø©ØŒ Ù†Ø­ÙØ¸ placeholder
    save_or_update_qa(user_input, "Ø³ÙŠØªÙ… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹")
    return "Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ø­Ø§Ù„ÙŠØ§Ù‹ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø£Ù† ØªØ®Ø¨Ø±Ù†ÙŠ Ø¨Ø§Ù„Ø±Ø¯ Ø§Ù„ØµØ­ÙŠØ­."
# --------------------------------------------
#  Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„ÙƒÙ„ Ù…Ø³ØªØ®Ø¯Ù…
# --------------------------------------------

user_memory = {}  # {session_id: [history_list]}

def add_to_memory(session_id, message, reply):
    """Ø­ÙØ¸ Ø¢Ø®Ø± ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    if session_id not in user_memory:
        user_memory[session_id] = []
    user_memory[session_id].append({"q": message, "a": reply})
    # Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø¢Ø®Ø± 5 ÙÙ‚Ø·
    if len(user_memory[session_id]) > 5:
        user_memory[session_id] = user_memory[session_id][-5:]

def get_memory_context(session_id):
    """Ø¥Ø±Ø¬Ø§Ø¹ Ø¢Ø®Ø± 3 Ø£Ø³Ø¦Ù„Ø© Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø°ÙƒØ§Ø¡"""
    if session_id not in user_memory:
        return ""
    history = user_memory[session_id][-3:]
    context = ""
    for h in history:
        context += f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {h['q']}\nØ§Ù„Ø¨ÙˆØª: {h['a']}\n"
    return context

# --------------------------------------------
#  API: Endpoint Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©
# --------------------------------------------

@app.route("/chat", methods=["POST"])
def chat():
    """Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„ØªØ®Ø§Ø·Ø¨ Ù…Ø¹ Ø§Ù„Ø¨ÙˆØª"""
    user_input = request.json.get("message", "")
    session_id = request.json.get("session_id", "default")  # Ù„Ùˆ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙŠÙ‡ Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù…ØªØ¹Ø¯Ø¯ÙŠÙ†

    candidate_answer = get_best_answer(user_input)

    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
    context = get_memory_context(session_id)

    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø±Ø¯ Ù…Ø¹ Ø£Ø®Ø° Ø§Ù„Ø³ÙŠØ§Ù‚ ÙÙŠ Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø±
    try:
        model = genai.GenerativeModel("models/gemini-2.5-pro")
        prompt = (
            "Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n" + context +
            f"\nØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ: {user_input}\n"
            f"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {candidate_answer}\n"
            "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚ Ù„ÙÙ‡Ù… Ø§Ù„Ù…Ù‚ØµÙˆØ¯ ÙˆØ±Ø¯ Ø¨Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ØªØ±Ø§Ø¨Ø·Ø©ØŒ Ù…Ø®ØªØµØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©."
        )
        response = model.generate_content(prompt)
        final_reply = response.text.strip().split("\n")[0]
    except Exception:
        final_reply = candidate_answer

    # Ø­ÙØ¸ Ø§Ù„ØªÙØ§Ø¹Ù„ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ù…Ù„Ù
    save_or_update_qa(user_input, final_reply)
    add_to_memory(session_id, user_input, final_reply)

    return jsonify({"reply": final_reply})


# --------------------------------------------
#  ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ±
# --------------------------------------------
if __name__ == "__main__":
    app.run(port=5000, debug=True)
